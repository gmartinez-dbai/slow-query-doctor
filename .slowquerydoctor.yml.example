# Slow Query Doctor Configuration Example
# Copy to .slowquerydoctor.yml and customize as needed

# AI Provider Configuration
llm_provider: ollama              # 'ollama' (local/private) or 'openai' (cloud/API key)
ollama_model: llama2             # Model name for Ollama (llama2, codellama, mistral, etc.)
ollama_host: http://localhost:11434  # Optional: Custom Ollama host (default: http://localhost:11434)

# OpenAI Configuration (alternative to Ollama)
# openai_api_key: your-api-key-here  # Can also use OPENAI_API_KEY env var
# openai_model: gpt-4o-mini          # OpenAI model to use

# Analysis Configuration
top_n: 5                         # Number of top slow queries to analyze
min_duration: 1000               # Minimum duration in ms to consider
output: reports/report.md        # Default output path

# LLM Configuration
llm_temperature: 0.3             # Temperature for AI responses (0.0-1.0)
max_tokens: 300                  # Maximum tokens for AI recommendations
llm_timeout: 30                  # Timeout in seconds for AI requests

# Log Format
log_format: plain                # 'plain', 'csv', or 'json'